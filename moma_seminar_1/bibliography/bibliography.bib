@article{Honerkamp2024,
  author    = {Daniel Honerkamp and Martin B{"u}chner and Fabien Despinoy and Tim Welschehold and Abhinav Valada},
  title     = {Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation},
  journal   = {IEEE Robotics and Automation Letters},
  year      = {2024},
  volume    = {9},
  number    = {2},
  pages     = {1--9},
  note      = {Preprint version},
}
% This file was created with JabRef 2.10.
% Encoding: UTF-8

@STRING{aaai    = {Proc.~of the Conference on Advancements of Artificial Intelligence (AAAI)} }
@STRING{aaaiold = {Proc.~of the National Conference on Artificial Intelligence (AAAI)} }
@STRING{ac      = {IEEE Trans. on Automatic Control} }
@STRING{acc     = {Proc.~of the IEEE American Control Conference (ACC)} }
@STRING{accv    = {Proc.~of the Asian Conf.~on Computer Vision (ACCV)} }
@STRING{acra    = {Proc.~of the Australasian Conf.~on Robotics and Automation (ACRA)} }
@STRING{acmgraphics = {ACM Transactions on Graphics} }
@STRING{addison = {Addison-Wesley Publishing Inc.} }
@STRING{advancedrobotics={Advanced Robotics} }
@STRING{ai      = {Artificial Intelligence} }
@STRING{ams     = {Proc.~of Autonome Mobile Systeme} }
@STRING{ar      = {Autonomous Robots} }
@STRING{arxiv   = {arXiv preprint} }
@STRING{bmvc    = {Proc.~of British Machine Vision Conference (BMVC)} }
@STRING{cacm    = {Communications of the ACM} }
@STRING{ccvw    = {Proc.~of the Croation Computer Vision Workshop (CCVW)} }
@STRING{cira    = {Proc.~of the IEEE Intl.~Symp. on Computer Intelligence in Robotics and Automation (CIRA)} }
@STRING{cogsys  = {Proc.~of the Intl.~Conf.~on Cognitive Systems (CogSys)} }
@STRING{cviu    = {Journal of Computer Vision and Image Understanding (CVIU)} }
@STRING{cvpr    = {Proc.~of the IEEE Conf.~on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvvt    = {Proc.~of the Intl.~Workshop on Computer Vision in Vehicle Technology (CVVT)} }
@STRING{dagm    = {Proc.~of the Symposion of the German Association for Pattern Recognition (DAGM)} }
@STRING{dagstuhl= {Proc.~of the Dagstuhl Seminar} }
@STRING{dgpf    = {Proc.~of the Conf.~of the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF)} }
@STRING{eccv    = {Proc.~of the Europ.~Conf.~on Computer Vision (ECCV)} }
@STRING{ecmr    = {Proc.~of the Europ.~Conf.~on Mobile Robotics (ECMR)} }
@STRING{emav    = {Proc.~of the European Micro Aerial Vehicle Conference} }
@STRING{euros   = {Proc.~of the Europ.~Robotics Symp. (EUROS)} }
@STRING{fntr    = {Foundations and Trends in Robotics} }
@STRING{gcpr    = {Proc.~of the German Conf.~on Pattern Recognition (GCPR)} }
@STRING{humanoids={Proc.~of the IEEE Intl.~Conf.~on Humanoid Robots} }
@STRING{icar    = {Proc.~of the Int.~Conf.~on Advanced Robotics (ICAR)} }
@STRING{iccv    = {Proc.~of the IEEE Intl.~Conf.~on Computer Vision (ICCV)} }
@STRING{iciap   = {Proc.~of the Intl.~Conf.~on Image Analysis and Processing (ICIAP)} }
@STRING{icip    = {Proc.~of the IEEE Intl.~Conf.~on Image Processing (ICIP)} }
@STRING{icra    = {Proc.~of the IEEE Intl.~Conf.~on Robotics \& Automation (ICRA)} }
@STRING{icuas   = {Proc.~of the Intl.~Conf.~on Unmanned Aircraft Systems (ICUAS)} }
@STRING{ieeepress={IEEE Computer Society Press} }
@STRING{ijcai   = {Proc.~of the Intl.~Conf.~on Artificial Intelligence (IJCAI)} }
@STRING{ijcv    = {Intl.~Journal~of Computer Vision (IJCV)} }
@STRING{ijgi    = {Intl.~Journal of Geo-Information} }
@STRING{ijhr    = {The Int.~Journal of Humanoid Robotics (IJHR)} }
@STRING{ijrr    = {Intl.~Journal~of Robotics Research (IJRR)} }
@STRING{imvip   = {Proc.~of the Irish Machine Vision and Image Processing Conference (IMVIP)} }
@STRING{iros    = {Proc.~of the IEEE/RSJ Intl.~Conf.~on Intelligent Robots and Systems (IROS)} }
@STRING{iser    = {Proc.~of the Intl.~Sym.~on Experimental Robotics (ISER)} }
@STRING{ismar   = {Proc.~of the Intl.~Symposium~on Mixed and Augmented Reality (ISMAR)} }
@STRING{isprsannals={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isprsarchives={ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isrr    = {Proc.~of the Intl.~Symposium~on Robotic Research (ISRR)} }
@STRING{iv      = {Proc.~of the IEEE Intelligent Vehicles Symposium (IV)} }
@STRING{ivc     = {Journal on Image and Vision Computing (IVC)} }
@STRING{jair    = {Journal of Artificial Intelligence Research (JAIR)} }
@STRING{jbe     = {ASME Journal of Basic Engineering} }
@STRING{jfr     = {Journal of Field Robotics (JFR)} }
@STRING{jirs    = {Journal of Intelligent and Robotic Systems (JIRS)} }
@STRING{jmiv    = {Journal of Mathematical Imaging and Vision} }
@STRING{joe     = {IEEE Journal of Oceanic Engineering} }
@STRING{jprs    = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)} }
@STRING{jra     = {IEEE Journal of Robotics and Automation} }
@STRING{jras    = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{mcg     = {Proc.~of the Intl.~Conf.~on Machine Control and Guidance (MCG)} }
@STRING{mirage  = {Proc.~of the Intl.~Conf.~on Computer Vision/Computer Graphics Collaboration Techniques and Applications (MIRAGE)} }
@STRING{mitpress= {MIT Press} }
@STRING{ml      = {Machine Learning} }
@STRING{mobicom = {Proc.~of the {ACM} Intl.~Conf.~on Mobile Computing and Networking (MobiCom)} }
@STRING{mva     = {Proc.~of the IAPR Conf.~on Machine Vision Applications (MVA)} }
@STRING{nips    = {Proc.~of the Conf.~on Neural Information Processing Systems (NIPS)} }
@STRING{nipsjournal={Advances in Neural Information Processing Systems} }
@STRING{oceans  = {Proc.~of OCEANS MTS/IEEE Conference and Exhibition} }
@STRING{pami    = {IEEE Trans.~on Pattern Analalysis and Machine Intelligence (TPAMI)} }
@STRING{pcv     = {Proc.~of the ISPRS Conference on Photogrammeric Computer Vision (PCV)} }
@STRING{pers    = {Photogrammetric Engineering and Remote Sensing (PE\&RS)} }
@STRING{pfg     = {Photogrammetrie -- Fernerkundung -- Geoinformation (PFG)} }
@STRING{phowo   = {Proc.~of the Photogrammetric Week (PhoWo)} }
@STRING{pia     = {Proc.~of the ISPRS Conference on Photogrammeric Image Analysis (PIA)} }
@STRING{pr      = {Pattern Recognition} }
@STRING{prl     = {Pattern Recognition Letters} }
@STRING{ral     = {IEEE Robotics and Automation Letters (RA-L)} }
@STRING{ram     = {IEEE Robotics and Automation Magazine (RAM)} }
@STRING{ras     = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{rasmag  = {IEEE Robotics and Automation Magazine} }
@STRING{rs      = {Remote Sensing} }
@STRING{rss     = {Proc.~of Robotics: Science and Systems (RSS)} }
@STRING{rssbook = {Robotics: Science and Systems} }
@STRING{sensors = {IEEE Sensors Journal} }
@STRING{sice    = {Proc.~of the Annual Conference of the Society of Instrument and Control Engineers (SICE)} }
@STRING{smc     = {Proc.~of the IEEE Intl.~Conf.~on Systems, Man, and Cybernetics (SMC)} }
@STRING{snowbird= {Proc.~of the Learning Workshop (Snowbird)} }
@STRING{soave   = {Proc.~of the Workshop on Self-Organization of AdaptiVE behavior (SOAVE)} }
@STRING{spiesdvrs={Proc.~of SPIE Stereoscopic Displays and Virtual Reality Systems} }
@STRING{spiev   = {Proc.~of SPIE Videometrics} }
@STRING{springer= {Springer Verlag} }
@STRING{springerstaradvanced={STAR Springer Tracts in Advanced Robotics} }
@STRING{tarj    = {The Australian Rangeland Journal} }
@STRING{tits    = {IEEE Trans.~on Intelligent Transportation Systems (ITS)} }
@STRING{titsmag = {IEEE Trans.~on Intelligent Transportation Systems Magazine} }
@STRING{tpami   = {IEEE Trans.~on Pattern Analalysis and Machine Intelligence (TPAMI)} }
@STRING{tra     = {IEEE Trans.~on Robotics and Automation} }
@STRING{tro     = {IEEE Trans.~on Robotics (TRO)} }
@STRING{uai     = {Proc.~of the Conf.~on Uncertainty in Artificial Intelligence (UAI)} }
@STRING{uavg    = {Proc.~of the Intl.~Conf.~on Unmanned Aerial Vehicles in Geomatics} }
@STRING{uust    = {Proc.~of the Intl.~Symp.~on Unmanned Untethered Submersible Technology} }
@STRING{vc      = {The Visual Computer (VC)} }
@STRING{wafr    = {Intl.~Workshop on the Algorithmic Foundations of Robotics (WAFR)} }


@Misc{platypusPicRef,
  Title                    = {Schnabeltier},
  Author                   = {Wikipedia},
  Note                     = {\url{http://de.wikipedia.org/wiki/Schnabeltier}, accessed on 7/10/2012.},
  Owner                    = {Admin},
  Timestamp                = {2015.12.06}
}

# ( ( "scene graph" OR "3D scene graph" ) AND ( "large language model" OR "LLM" OR "VLM" OR "language model" OR "semantic reasoning" ) AND ( "mobile manipulation" OR "task planning" OR "object search" OR "scene understanding" ) ) OR ( ( "open-vocabulary" OR "zero-shot" ) AND ( "mobile manipulation" OR "pick and place" OR "pick-and-place" OR "search and retrieval" ) AND ( "language grounding" OR "command grounding" OR "natural language" OR "semantic reasoning" ) ) OR ( ( "unseen object" OR "novel object" OR "open-vocabulary" ) AND ( "robotic grasping" OR "grasp detection" OR "grasp planning" ) AND ( "language" OR "text" OR "language-grounded" ) ) OR ( ( "natural language instruction" OR "text-based command" ) AND ( "robot-agnostic" OR "task planning" ) AND ( "motion planning" OR "robot action" OR "manipulation" ) ) OR ( ( "vision-language model" OR "VLM" OR "language model" ) AND ( "mobile manipulation" OR "robot manipulation" ) AND ( "pick and place" OR "grasping" OR "robot task" OR "search and retrieval" ) ) OR ( ( "active perception" OR "interactive search" OR "active vision" OR "next best view" OR "viewpoint planning" ) AND ( "large language model" OR "LLM" OR "VLM" OR "language model" OR "semantic reasoning" ) AND ( "robotics" OR "mobile manipulation" OR "object search" OR "scene understanding" OR "task planning" ) )

# ( "scene graph" OR "3D scene graph" ) AND ( "large language model" OR "LLM" OR "language-grounded" ) AND ( "mobile manipulation" OR "task planning" OR "object search" )

@ARTICLE{10632580,
  author={Honerkamp, Daniel and Büchner, Martin and Despinoy, Fabien and Welschehold, Tim and Valada, Abhinav},
  journal={IEEE Robotics and Automation Letters}, 
  title={Language-Grounded Dynamic Scene Graphs for Interactive Object Search With Mobile Manipulation}, 
  year={2024},
  volume={9},
  number={10},
  pages={8298-8305},
  abstract={To fully leverage the capabilities of mobile manipulation robots, it is imperative that they are able to autonomously execute long-horizon tasks in large unexplored environments. While large language models (LLMs) have shown emergent reasoning skills on arbitrary tasks, existing work primarily concentrates on explored environments, typically focusing on either navigation or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel approach that grounds language models within structured representations derived from open-vocabulary scene graphs, dynamically updated as the environment is explored. We tightly interleave these representations with an object-centric action space. Given object detections, the resulting approach is zero-shot, open-vocabulary, and readily extendable to a spectrum of mobile manipulation and household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive search task in large realistic indoor environments. In extensive experiments in both simulation and the real world, we show substantially improved search efficiency compared to conventional baselines and state-of-the-art approaches, as well as its applicability to more abstract tasks.},
  keywords={Task analysis;Semantics;Cognition;Search problems;Navigation;Planning;Three-dimensional displays;Scene graphs;decision making;object search},
  doi={10.1109/LRA.2024.3441495},
  ISSN={2377-3766},
  month={Oct},}

@INPROCEEDINGS{11128343,
  author={Kwon, Seokjoon and Park, Jae-Hyeon and Jang, Hee-Deok and Roh, CheolLae and Chang, Dong Eui},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Large Language Model Based Autonomous Task Planning for Abstract Commands}, 
  year={2025},
  volume={},
  number={},
  pages={11010-11016},
  abstract={Recent advances in large language models (LLMs) have demonstrated exceptional reasoning capabilities in natural language processing, sparking interest in applying LLMs to task planning problems in robotics. Most studies focused on task planning for clear natural language commands that specify target objects and their locations. However, for more user-friendly task execution, it is crucial for robots to autonomously plan and carry out tasks based on abstract natural language commands that may not explicitly mention target objects or locations, such as ‘Put the food ingredients in the same place.’ In this study, we propose an LLM-based autonomous task planning framework that generates task plans for abstract natural language commands. This framework consists of two phases: an environment recognition phase and a task planning phase. In the environment recognition phase, a large vision-language model generates a hierarchical scene graph that captures the relationships between objects and spaces in the environment surrounding a robot agent. During the task planning phase, an LLM uses the scene graph and the abstract user command to formulate a plan for the given task. We validate the effectiveness of the proposed framework in the AI2THOR simulation environment, demonstrating its superior performance in task execution when handling abstract commands.},
  keywords={Large language models;Semantics;Real-time systems;Natural language processing;Cognition;Planning;Robots},
  doi={10.1109/ICRA55743.2025.11128343},
  ISSN={},
  month={May},}
@INPROCEEDINGS{10610599,
  author={Dai, Zhirui and Asgharivaskasi, Arash and Duong, Thai and Lin, Shusen and Tzes, Maria-Elizabeth and Pappas, George and Atanasov, Nikolay},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Optimal Scene Graph Planning with Large Language Model Guidance}, 
  year={2024},
  volume={},
  number={},
  pages={14062-14069},
  abstract={Recent advances in metric, semantic, and topological mapping have equipped autonomous robots with concept grounding capabilities to interpret natural language tasks. Leveraging these capabilities, this work develops an efficient task planning algorithm for hierarchical metric-semantic models. We consider a scene graph model of the environment and utilize a large language model (LLM) to convert a natural language task into a linear temporal logic (LTL) automaton. Our main contribution is to enable optimal hierarchical LTL planning with LLM guidance over scene graphs. To achieve efficiency, we construct a hierarchical planning domain that captures the attributes and connectivity of the scene graph and the task automaton, and provide semantic guidance via an LLM heuristic function. To guarantee optimality, we design an LTL heuristic function that is provably consistent and supplements the potentially inadmissible LLM guidance in multi-heuristic planning. We demonstrate efficient planning of complex natural language tasks in scene graphs of virtualized real environments.},
  keywords={Measurement;Grounding;Large language models;Semantics;Automata;Planning;Logic},
  doi={10.1109/ICRA57147.2024.10610599},
  ISSN={},
  month={May},}
@INPROCEEDINGS{10801291,
  author={Ni, Zhe and Deng, Xiaoxin and Tai, Cong and Zhu, Xinyue and Xie, Qinghongbing and Huang, Weihang and Wu, Xiang and Zeng, Long},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={GRID: Scene-Graph-based Instruction-driven Robotic Task Planning}, 
  year={2024},
  volume={},
  number={},
  pages={13765-13772},
  abstract={Recent works have shown that Large Language Models (LLMs) can facilitate the grounding of instructions for robotic task planning. Despite this progress, most existing works have primarily focused on utilizing raw images to aid LLMs in understanding environmental information. However, this approach not only limits the scope of observation but also typically necessitates extensive multimodal data collection and large-scale models. In this paper, we propose a novel approach called Graph-based Robotic Instruction Decomposer (GRID), which leverages scene graphs instead of images to perceive global scene information and iteratively plan subtasks for a given instruction. Our method encodes object attributes and relationships in graphs through an LLM and Graph Attention Networks, integrating instruction features to predict subtasks consisting of pre-defined robot actions and target objects in the scene graph. This strategy enables robots to acquire semantic knowledge widely observed in the environment from the scene graph. To train and evaluate GRID, we establish a dataset construction pipeline to generate synthetic datasets for graph-based robotic task planning. Experiments have shown that our method outperforms GPT-4 by over 25.4% in subtask accuracy and 43.6% in task accuracy. Moreover, our method achieves a real-time speed of 0.11s per inference. Experiments conducted on datasets of unseen scenes and scenes with varying numbers of objects demonstrate that the task accuracy of GRID declined by at most 3.8%, showcasing its robust cross-scene generalization ability. We validate our method in both physical simulation and the real world. More details can be found on the project page https://jackyzengl.github.io/GRID.github.io/.},
  keywords={Accuracy;Grounding;Large language models;Semantics;Pipelines;Real-time systems;Planning;Robots;Intelligent robots;Synthetic data},
  doi={10.1109/IROS58592.2024.10801291},
  ISSN={2153-0866},
  month={Oct},}

@INPROCEEDINGS{11128414,
  author={Rivera, Corban and Byrd, Grayson and Paul, William and Feldman, Tyler and Booker, Meghan and Holmes, Emma and Handelman, David and Kemp, Bethany and Badger, Andrew and Schmidt, Aurora and Jatavallabhula, Krishna Murthy and de Melo, Celso M and Seenivasan, Lalithkumar and Unberath, Mathias and Chellappa, Rama},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={ConceptAgent: LLM-Driven Precondition Grounding and Tree Search for Robust Task Planning and Execution}, 
  year={2025},
  volume={},
  number={},
  pages={8988-8995},
  abstract={Robotic planning and execution in open-world environments is a complex problem due to the vast state spaces and high variability of task embodiment. Recent advances in perception algorithms, combined with Large Language Models (LLMs) for planning, offer promising solutions to these challenges, as the common sense reasoning capabilities of LLMs provide a strong heuristic for efficiently searching the action space. However, prior work fails to address the possibility of hallucinations from LLMs, which results in failures to execute the planned actions largely due to logical fallacies at high-or low-levels. To contend with automation failure due to such hallucinations, we introduce ConceptAgent, a natural language-driven robotic platform designed for task execution in unstructured environments. With a focus on scalability and reliability of LLM-based planning in complex state and action spaces, we present innovations designed to limit these shortcomings, including 1) Predicate Grounding to prevent and recover from infeasible actions, and 2) an embodied version of LLM-guided Monte Carlo Tree Search with self reflection. ConceptAgent combines these planning enhancements with dynamic language aligned 3d scene graphs, and large multi-modal pretrained models to perceive, localize, and interact with its environment, enabling reliable task completion. In simulation experiments, ConceptAgent achieved a 19% task completion rate across three room layouts and 30 easy level embodied tasks outperforming other state-of-the-art LLM-driven reasoning baselines that scored 10.26% and 8.11% on the same benchmark. Additionally, ablation studies on moderate to hard embodied tasks revealed a 20% increase in task completion from the baseline agent to the fully enhanced ConceptAgent, highlighting the individual and combined contributions of Predicate Grounding and LLM-guided Tree Search to enable more robust automation in complex state and action spaces. Additionally, in real-world mobile manipulation trials, conducted in randomized, low-clutter environments, a ConceptAgent-driven Spot robot achieved a 40% task completion rate, demonstrating the performance of our perception system in real-world scenarios.},
  keywords={Adaptation models;Visualization;Technological innovation;Solid modeling;Automation;Three-dimensional displays;Grounding;Planning;Software reliability;Clutter},
  doi={10.1109/ICRA55743.2025.11128414},
  ISSN={},
  month={May},}
@ARTICLE{11106512,
  author={Li, Zhongyang and Lu, Fei and Fu, Tengfan and Tian, Guohui},
  journal={IEEE Robotics and Automation Letters}, 
  title={From Demand to Grounded Plan: Task Customization and Planning for Service Robots With Deep Learning and LLMs}, 
  year={2025},
  volume={10},
  number={9},
  pages={9526-9533},
  abstract={In the field of task planning for service robots, large language model (LLM)-based approaches have shown increasing potential but still struggle with responding to complex user demands and grounded task planning. In this letter, we propose a user-demand-oriented adaptive grounded task planning system that integrates three modules: User-Centric Adaptive Task Customization (UATC), Task-Driven Scene Graph Pruning (TSGP), and Grounded Task Planning (GTP). UATC utilizes semantic demand-task contrastive learning to develop a novel demand parsing model for user natural language, generating personalized tasks that align with user needs and environmental constraints. TSGP employs LLM-based scene graph pruning and recursive traversal strategies to extract task-relevant environmental information. GTP generates an initial task plan using LLMs through action selection and effect judgment, followed by a three-stage refinement incorporating environmental constraints to ensure precise grounding of actions. Experimental results demonstrate that our approach can customize executable tasks in response to diverse user demands, and achieves strong planning performance with an executability rate of 91.39% and a task success rate of 78.33% in VirtualHome, outperforming existing state-of-the-art baselines. Moreover, a real-world deployment on the TIAGo robot further validates the system's applicability in physical environments.},
  keywords={Planning;Semantics;Contrastive learning;Natural languages;Adaptation models;Service robots;Encoding;Bidirectional control;Data mining;Cognition;Task planning;service robotics},
  doi={10.1109/LRA.2025.3595029},
  ISSN={2377-3766},
  month={Sep.},}
@INPROCEEDINGS{11127838,
  author={Liu, Yuchen and Palmieri, Luigi and Koch, Sebastian and Georgievski, Ilche and Aiello, Marco},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={DELTA: Decomposed Efficient Long-Term Robot Task Planning Using Large Language Models}, 
  year={2025},
  volume={},
  number={},
  pages={10995-11001},
  abstract={Recent advancements in Large Language Models (LLMs) have sparked a revolution across many research fields. In robotics, the integration of common-sense knowledge from LLMs into task and motion planning has drastically advanced the field by unlocking unprecedented levels of context awareness. Despite their vast collection of knowledge, large language models may generate infeasible plans due to hal-lucinations or missing domain information. To address these challenges and improve plan feasibility and computational efficiency, we introduce DELTA, a novel LLM-informed task planning approach. By using scene graphs as environment representations within LLMs, DELTA achieves rapid generation of precise planning problem descriptions. To enhance planning performance, DELTA decomposes long-term task goals with LLMs into an autoregressive sequence of sub-goals, enabling automated task planners to efficiently solve complex problems. In our extensive evaluation, we show that DELTA enables an efficient and fully automatic task planning pipeline, achieving higher planning success rates and significantly shorter planning times compared to the state of the art. Project webpage: https://delta-llm.github.io/},
  keywords={Training;Uncertainty;Large language models;Semantics;Pipelines;Context awareness;Planning;Computational efficiency;Robots;Commonsense reasoning},
  doi={10.1109/ICRA55743.2025.11127838},
  ISSN={},
  month={May},}
@INPROCEEDINGS{11127555,
  author={Guo, Weihang and Kingston, Zachary and Kavraki, Lydia E.},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={CaStL: Constraints as Specifications Through Llm Translation for Long-Horizon Task and Motion Planning}, 
  year={2025},
  volume={},
  number={},
  pages={11957-11964},
  abstract={Large Language Models (LLMs) have demonstrated remarkable ability in long-horizon Task and Motion Planning (TAMP) by translating clear and straightforward natural language problems into formal specifications such as the Planning Domain Definition Language (PDDL). However, real-world problems are often ambiguous and involve many complex constraints. In this paper, we introduce Constraints as Specifications through LLMs (CaStL), a framework that identifies constraints such as goal conditions, action ordering, and action blocking from natural language in multiple stages. CaStL translates these constraints into PDDL and Python scripts, which are then solved using an custom PDDL solver. Tested across three PDDL domains, CaStL significantly improves constraint handling and planning success rates from natural language specification in complex scenarios.},
  keywords={Constraint handling;Translation;Uncertainty;Large language models;Planning;Formal specifications;Robotics and automation},
  doi={10.1109/ICRA55743.2025.11127555},
  ISSN={},
  month={May},}

@INPROCEEDINGS{10161534,
  author={Chen, Boyuan and Xia, Fei and Ichter, Brian and Rao, Kanishka and Gopalakrishnan, Keerthana and Ryoo, Michael S. and Stone, Austin and Kappler, Daniel},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Open-vocabulary Queryable Scene Representations for Real World Planning}, 
  year={2023},
  volume={},
  number={},
  pages={11509-11522},
  abstract={Large language models (LLMs) have unlocked new capabilities of task planning from human instructions. However, prior attempts to apply LLMs to real-world robotic tasks are limited by the lack of grounding in the surrounding scene. In this paper, we develop NLMap, an open-vocabulary and queryable scene representation to address this problem. NLMap serves as a framework to gather and integrate contextual information into LLM planners, allowing them to see and query available objects in the scene before generating a context-conditioned plan. NLMap first establishes a natural language queryable scene representation with Visual Language models (VLMs). An LLM based object proposal module parses instructions and proposes involved objects to query the scene representation for object availability and location. An LLM planner then plans with such information about the scene. NLMap allows robots to operate without a fixed list of objects nor executable options, enabling real robot operation unachievable by previous methods. Project website: https://nlmap-saycan.github.io.},
  keywords={Visualization;Automation;Grounding;Natural languages;Planning;Proposals;Task analysis},
  doi={10.1109/ICRA48891.2023.10161534},
  ISSN={},
  month={May},}



# ( "open-vocabulary" OR "zero-shot" ) AND ( "mobile manipulation" OR "pick and place" OR "pick-and-place" ) AND ( "language grounding" OR "command grounding" OR "natural language" )


@INPROCEEDINGS{11127619,
  author={Liu, Peiqi and Guo, Zhanqiu and Warke, Mohit and Chintala, Soumith and Paxton, Chris and Shafiullah, Nur Muhammad Mahi and Pinto, Lerrel},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Dynamem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation}, 
  year={2025},
  volume={},
  number={},
  pages={13346-13355},
  abstract={Significant progress has been made in openvocabulary mobile manipulation, where the goal is for a robot to perform tasks in any environment given a natural language description. However, most current systems assume a static environment, which limits the system's applicability in realworld scenarios where environments frequently change due to human intervention or the robot's own actions. In this work, we present DynaMem, a new approach to open-world mobile manipulation that uses a dynamic spatio-semantic memory to represent a robot's environment. DynaMem constructs a 3D data structure to maintain a dynamic memory of point clouds, and answers open-vocabulary object localization queries using multimodal LLMs or open-vocabulary features generated by state-of-the-art vision-language models. Powered by DynaMem, our robots can explore novel environments, search for objects not found in memory, and continuously update the memory as objects move, appear, or disappear in the scene. We run extensive experiments on the Stretch SE3 robots in three real and nine offline scenes, and achieve an average pick-and-drop success rate of 70 % on non-stationary objects, which is more than a $\mathbf{2} \times$ improvement over state-of-the-art static systems.},
  keywords={Point cloud compression;Location awareness;Solid modeling;Three-dimensional displays;Natural languages;Search problems;Data structures;Data models;Robots;Manipulator dynamics},
  doi={10.1109/ICRA55743.2025.11127619},
  ISSN={},
  month={May},}@INPROCEEDINGS{11127975,
  author={Zhi, Peiyuan and Zhang, Zhiyuan and Zhao, Yu and Han, Muzhi and Zhang, Zeyu and Li, Zhitian and Jiao, Ziyuan and Jia, Baoxiong and Huang, Siyuan},
  booktitle={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Closed-Loop Open-Vocabulary Mobile Manipulation with GPT-4V}, 
  year={2025},
  volume={},
  number={},
  pages={4761-4767},
  abstract={Autonomous robot navigation and manipulation in open environments require reasoning and replanning with closed-loop feedback. In this work, we present COME-robot, the first closed-loop robotic system utilizing the GPT-4V vision-language foundation model for open-ended reasoning and adaptive planning in real-world scenarios. COME-robot incorporates two key innovative modules: (i) a multi-level open-vocabulary perception and situated reasoning module that enables effective exploration of the 3D environment and target object identification using commonsense knowledge and situated information, and (ii) an iterative closed-loop feedback and restoration mechanism that verifies task feasibility, monitors execution success, and traces failure causes across different modules for robust failure recovery. Through comprehensive experiments involving 8 challenging real-world mobile and tabletop manipulation tasks, COME-robot demonstrates a significant improvement in task success rate ($\sim 35 \%$) compared to state-of-the-art methods. We further conduct comprehensive analyses to elucidate how COME-robot's design facilitates failure recovery, free-form instruction following, and long-horizon task planning.},
  keywords={Three-dimensional displays;Adaptive systems;Foundation models;Navigation;Planning;Object recognition;Iterative methods;Monitoring;Commonsense reasoning;Autonomous robots},
  doi={10.1109/ICRA55743.2025.11127975},
  ISSN={},
  month={May},}

# ( "unseen object" OR "novel object" OR "open-vocabulary" ) AND ( "robotic grasping" OR "grasp detection" OR "grasp planning" ) AND ( "language" OR "text" )


# ( "natural language instruction" OR "text-based command" ) AND ( "robot-agnostic" OR "task planning" ) AND ( "motion planning" OR "robot action" OR "manipulation" )

@ARTICLE{10097850,
  author={Zhao, Chao and Yuan, Shuai and Jiang, Chunli and Cai, Junhao and Yu, Hongyu and Wang, Michael Yu and Chen, Qifeng},
  journal={IEEE Robotics and Automation Letters}, 
  title={ERRA: An Embodied Representation and Reasoning Architecture for Long-Horizon Language-Conditioned Manipulation Tasks}, 
  year={2023},
  volume={8},
  number={6},
  pages={3230-3237},
  abstract={This letter introduces ERRA, an embodied learning architecture that enables robots to jointly obtain three fundamental capabilities (reasoning, planning, and interaction) for solving long-horizon language-conditioned manipulation tasks. ERRA is based on tightly-coupled probabilistic inferences at two granularity levels. Coarse-resolution inference is formulated as sequence generation through a large language model, which infers action language from natural language instruction and environment state. The robot then zooms to the fine-resolution inference part to perform the concrete action corresponding to the action language. Fine-resolution inference is constructed as a Markov decision process, which takes action language and environmental sensing as observations and outputs the action. The results of action execution in environments provide feedback for subsequent coarse-resolution reasoning. Such coarse-to-fine inference allows the robot to decompose and achieve long-horizon tasks interactively. In extensive experiments, we show that ERRA can complete various long-horizon manipulation tasks specified by abstract language instructions. We also demonstrate successful generalization to the novel but similar natural language instructions.},
  keywords={Task analysis;Robots;Planning;Concrete;Cognition;Reinforcement learning;Natural languages;Manipulation;large language model (LLM);reasoning;reinforcement learning;human-robot interaction},
  doi={10.1109/LRA.2023.3265893},
  ISSN={2377-3766},
  month={June},}

# ( "vision-language model" OR "VLM" ) AND ( "mobile manipulation" OR "robot manipulation" ) AND ( "pick and place" OR "grasping" OR "robot task" )


