\subsection{Experiments}
\label{sec:experiments}

\subsubsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Simulator:} iGibson (based on real-world scans) \cite{ref1}.
    \item \textbf{Robot (Sim):} Fetch mobile manipulator \cite{ref2}.
    \item \textbf{Robot (Real):} Toyota HSR \cite{ref3}.
\end{itemize}

\subsubsection{Evaluation Metrics}
\begin{itemize}
    \item \textbf{Success Rate (SR):} Percentage of episodes where the target object is successfully found and \texttt{done()} is called.
    \item \textbf{SPL:} Success weighted by Path Length, measuring navigation efficiency.
    \item \textbf{AUC-E (Area Under Efficiency Curve):} A novel metric introduced to evaluate search efficiency across varying time budgets, providing a scalar value that rewards fast and reliable agents, unlike binary cutoffs \cite{ref8}.
\end{itemize}

\subsubsection{Results}

\textbf{Simulation}
\begin{itemize}
    \item \textbf{MoMa-LLM vs Baselines:} MoMa-LLM achieved the highest performance (SR 97.7\%, AUC-E 87.2\%), significantly outperforming unstructured LLM baselines and RL-based methods like HIMOS (SPL 48.5) \cite{ref13}.
    \item \textbf{Failure Modes:} Primary failures were due to perception limitations or "infeasible actions" generated by the LLM when context was overwhelmed, though structured grounding reduced this significantly compared to baselines \cite{ref15}.
\end{itemize}

\textbf{Real-world Transfer}
\begin{itemize}
    \item \textbf{Transfer Capabilities:} The high-level reasoning transferred effectively to the real world (80\% success rate), showing robustness to domain shifts in perception \cite{ref18}.
    \item \textbf{Latency and Constraints:} Main constraints involved the computational latency of LLM queries and the reliance on specific detectors (like AR markers) to handle real-world perception noise \cite{ref19}.
\end{itemize}
