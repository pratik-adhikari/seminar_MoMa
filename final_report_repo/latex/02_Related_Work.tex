\section{Related Work}
\label{sec:related_work}

Here is the summary of the related work categorized by the gaps MoMa-LLM addresses:

\subsection{3D Scene Graphs}
\begin{itemize}
    \item \textbf{Existing Approaches:} 3D scene graphs abstract dense maps into hierarchical, object-centric representations, with works like Hydra focusing on representing dynamically changing scenes \cite{ref1}. Other approaches, such as ConceptGraphs and VoroNav, investigate zero-shot perception inputs for task planning \cite{ref2}.
    \item \textbf{MoMa-LLM Differentiation:} While previous works utilize scene graphs for reasoning, they have not realized object navigation using both dynamic and interactive scene graphs \cite{ref3}. MoMa-LLM constructs open-vocabulary scene graphs dynamically from dense maps and Voronoi graphs, tightly interleaving them with an object-centric action space as the environment is explored \cite{ref4}.
\end{itemize}

\subsection{LLMs for Planning}
\begin{itemize}
    \item \textbf{Existing Approaches:} Models like LLM-Planner use information retrieval for planning, while SayPlan focuses on identifying subgraphs within large, known scene graphs \cite{ref5}. Most efforts focus on fully observed environments (e.g., table-top manipulation) or a priori explored scenes \cite{ref6}.
    \item \textbf{MoMa-LLM Differentiation:} Current methods struggle to generate grounded, executable plans for partially observed or unexplored environments \cite{ref7}. MoMa-LLM addresses this by grounding LLMs in dynamically built graphs, enabling reasoning over long horizons in fully unexplored scenes where simple prompting strategies are insufficient \cite{ref8}.
\end{itemize}

\subsection{Object Search}
\begin{itemize}
    \item \textbf{Existing Approaches:} Classical methods include frontier exploration and reinforcement learning, while recent semantic methods like ESC use language models to score frontiers based on object co-occurrences \cite{ref9}. SayNav utilizes scene graphs but relies on non-interactive search and hardcoded heuristics for navigation \cite{ref10}.
    \item \textbf{MoMa-LLM Differentiation:} Unlike methods that rely on pairwise scoring or directional reasoning, MoMa-LLM treats search as a planning problem where the full scene is encoded jointly \cite{ref11}. It specifically fills the gap of interactive search, allowing the agent to reason about manipulating the environment (e.g., opening doors or receptacles) to find objects that are not freely accessible \cite{ref12}.
\end{itemize}

\subsection{Structured Knowledge Representation in MoMa-LLM}
To effectively use Large Language Models (LLMs) for robotic planning, MoMa-LLM converts complex 3D scene graphs into a structured textual format. This process bridges the gap between the robot's geometric understanding and the LLM's reasoning capabilities, focusing on three properties: grounding (adherence to physical reality), specificity (avoiding irrelevant context), and open-set reasoning (handling unknown semantics) \cite{ref1}.

The representation is composed of four key elements:

\subsubsection{Scene Structure Encoding}
Rather than feeding raw data (like JSON) to the LLM, the system extracts a structured list of rooms and objects designed for compact text encoding \cite{ref2}.
\begin{itemize}
    \item \textbf{Distance Abstraction:} Exact metric distances are binned and mapped to natural language adjectives (e.g., "near", "far") to facilitate easier reasoning for the LLM \cite{ref3}.
    \item \textbf{Object States:} The state of an object is encoded directly into its name (e.g., "opened door" or "closed fridge") to provide immediate context on interactability \cite{ref4}.
    \item \textbf{Summarization:} To keep the context concise, matching nodes are summarized with counters, and open doors that do not provide new connectivity are filtered out \cite{ref5}.
\end{itemize}

\subsubsection{Handling Partial Observability}
Since the environment is initially unknown, the representation explicitly encodes "frontiers" (boundaries between explored and unexplored space) to enable exploration-exploitation reasoning \cite{ref6}.
\begin{itemize}
    \item \textbf{Frontier Types:} The system differentiates between two types of unknown space:
    \begin{itemize}
        \item "Unexplored Area": Space within a room (e.g., occluded space behind furniture) \cite{ref7}.
        \item Leading Out: Frontiers that lead to entirely new areas or rooms, which are listed separately \cite{ref8}.
    \end{itemize}
    \item \textbf{Affordances:} Instead of explicitly listing what every object can do, the system relies on the LLM to infer affordances (e.g., that a fridge can be opened) from the object descriptions and states \cite{ref9}.
\end{itemize}

\subsubsection{Dynamic History Realignment}
To maintain a valid history in a constantly changing map (where a "living room" might later be reclassified as a "kitchen"), the system uses a novel realignment mechanism \cite{ref10}.
\begin{itemize}
    \item \textbf{Position Tracking:} The system tracks the physical interaction positions of previous actions \cite{ref11}.
    \item \textbf{Retrospective Updates:} When constructing the prompt, previous actions are re-mapped to the current scene graph. For example, if a room was previously called "living room" but is now classified as "kitchen," the history will display \texttt{explore(kitchen)} instead of the original \texttt{explore(living room)} \cite{ref12}.
\end{itemize}

\subsubsection{Re-trial and Feedback}
To handle failures without overwhelming the context window, the system provides simplified feedback.
\begin{itemize}
    \item \textbf{State Feedback:} The LLM receives simple status updates like "success", "failure", or "invalid argument" \cite{ref13}.
    \item \textbf{Replanning:} If a command is syntactically invalid or infeasible, the LLM is prompted to try again immediately. If a physical action fails during execution, the scene is re-encoded with the new state, and the LLM makes a fresh decision \cite{ref14}.
\end{itemize}
