\section{Introduction}
\label{sec:introduction}

Interactive embodied AI tasks in large, human-centered environments require robots to reason over long horizons and interact with a multitude of objects \citetip{ref7}{MoMa-LLM (Honerkamp et al., 2024)}. In many real-world scenarios, these environments are a priori unknown or continuously rearranged, making autonomous operation significantly more difficult \citetip{ref8}{MoMa-LLM - OpenReview}. Specifically, the task of interactive object search presents unique challenges because objects are not always openly visible; they may be stored inside receptacles like cabinets or drawers, or located behind closed doors \citetip{ref9}{MoMa-LLM - RSS Workshop}. Consequently, an agent cannot rely on directional reasoning alone but must actively manipulate the environment—opening doors to navigate and searching through containers—to succeed \citetip{ref10}{LookPlanGraph}.

Recent advancements have shown the potential of Large Language Models (LLMs) for generating high-level robotic plans \citetip{ref11}{ESC - arXiv}. However, existing methods such as SayCan or SayPlan are insufficient for interactive search in unexplored settings for several reasons:

\begin{itemize}
    \item \textbf{Assumption of Full Observability:} Many prior works focus on fully observed environments, such as tabletop manipulation or pre-explored scenes \citetip{ref12}{MoMa-LLM - Scribd}.
    \item \textbf{Scalability and Hallucination:} In large, partially observable scenes with numerous objects, simply providing an LLM with raw observations or lists of objects increases the risk of generating impractical sequences or hallucinations \citetip{ref13}{N2M2}. Simple prompting strategies or raw JSON inputs of full scene graphs have proven insufficient for complex reasoning in these contexts \citetip{ref14}{MoMa-LLM - Emergent Mind}.
    \item \textbf{Limited Scope:} Methods often restrict tasks to single rooms or rely on non-interactive navigation, failing to address the complexities of multi-room exploration and physical interaction \citetip{ref15}{Inter-LLM}.
\end{itemize}

\subsection{Proposed Solution}
To address these limitations, the authors propose MoMa-LLM, a method that grounds LLMs in dynamically built scene graphs \citetip{ref16}{Inter-LLM - Abstract}. This approach utilizes a scene understanding module that constructs open-vocabulary scene graphs from dense maps and Voronoi graphs as the robot explores \citetip{ref17}{Explainable Saliency (Chen et al., 2025)}. By extracting structured and compact textual representations from these dynamic graphs, the system enables pre-trained LLMs to plan efficiently in partially observable environments \citetip{ref18}{Awesome-Robotics-3D}. This allows the robot to perform zero-shot, open-vocabulary reasoning, extending readily to a spectrum of complex mobile manipulation tasks \citetip{ref19}{Open-Vocabulary Search}.
