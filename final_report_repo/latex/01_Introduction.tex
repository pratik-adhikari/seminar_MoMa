\section{Introduction}
\label{sec:introduction}

Interactive embodied AI tasks in large, human-centered environments require robots to reason over long horizons and interact with a multitude of objects \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}. In many real-world scenarios, these environments are a priori unknown or continuously rearranged, making autonomous operation significantly more difficult \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}. Specifically, the task of interactive object search presents unique challenges because objects are not always openly visible; they may be stored inside receptacles like cabinets or drawers, or located behind closed doors \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}. Consequently, an agent cannot rely on directional reasoning alone but must actively manipulate the environment—opening doors to navigate and searching through containers—to succeed \citetip{himos}{HIMOS (Schmalstieg et al., 2023)}.

Recent advancements have shown the potential of Large Language Models (LLMs) for generating high-level robotic plans \citetip{saycan}{SayCan (Ahn et al., 2022)}. However, existing methods such as SayCan or SayPlan are insufficient for interactive search in unexplored settings for several reasons:

\begin{itemize}
    \item \textbf{Assumption of Full Observability:} Many prior works focus on fully observed environments, such as tabletop manipulation or pre-explored scenes \citetip{sayplan}{SayPlan (Rana et al., 2023)}.
    \item \textbf{Scalability and Hallucination:} In large, partially observable scenes with numerous objects, simply providing an LLM with raw observations or lists of objects increases the risk of generating impractical sequences or hallucinations \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}.
    \item \textbf{Limited Scope:} Methods often restrict tasks to single rooms or rely on non-interactive navigation, failing to address the complexities of multi-room exploration and physical interaction \citetip{himos}{HIMOS (Schmalstieg et al., 2023)}.
\end{itemize}

\subsection{Proposed Solution}
To address these limitations, the authors propose MoMa-LLM, a method that grounds LLMs in dynamically built scene graphs \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}. This approach utilizes a scene understanding module that constructs open-vocabulary scene graphs from dense maps and Voronoi graphs as the robot explores \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}. By extracting structured and compact textual representations from these dynamic graphs, the system enables pre-trained LLMs to plan efficiently in partially observable environments \citetip{moma_llm}{MoMa-LLM (Honerkamp et al., 2024)}.
