## 5. Conclusion

In this work, we presented MoMa-LLM, a novel and unified framework designed to ground Large Language Models (LLMs) within dynamically constructed, open-vocabulary scene graphs [moma_llm]. By tightly interleaving high-level semantic reasoning with object-centric low-level policies, our approach allows mobile manipulation robots to effectively explore, navigate, and interact with large, initially unknown environments. We demonstrated that extracting structured knowledge—incorporating spatial constraints and exploration history—is critical for enabling LLMs to generate feasible and efficient plans in complex real-world scenarios.

Through extensive evaluation in both simulation [igibson] and the real world, we showed that MoMa-LLM significantly outperforms conventional heuristics, reinforcement learning methods like HIMOS [himos], and unstructured LLM baselines in terms of search efficiency and decision-making capabilities. Furthermore, our results highlight the framework's flexibility, proving it is readily extendable to abstract "fuzzy" search tasks and general household activities beyond simple object retrieval.

Future work lies in cost-aware grounding and integrating VLM-based room classification to further improve robustness in noisy real-world perception scenarios [moma_llm].