\chapter{Related Work}
\label{ch:related_work}

Interactive object search sits at the intersection of exploration, semantic mapping, manipulation and language understanding.  Classical work on object search focused on enabling robots to navigate through known environments and identify objects using computer vision; these methods often assumed that target objects were visible from afar and ignored the need for manipulation.  Recent methods such as Semi‑Autonomous Next‑Best‑Object Selection and RL‑based object search introduce learned policies for deciding where to look next, but they still operate on fixed maps and predefined action spaces.  In contrast, \emph{interactive} search acknowledges that many everyday objects are hidden behind cabinet doors or inside drawers and therefore requires the robot to interact with the environment.

The use of large language models as planners in robotics has been explored in frameworks such as \gls{react}, which alternates between reasoning and acting via prompts, and Chain‑of‑Thought planning.  Other works embed state information into free‑text prompts that instruct the LLM to select discrete actions.  These approaches show promise for zero‑shot task execution but often neglect the physical constraints of the robot and lack an explicit representation of the environment.  MoMa‑LLM addresses this by grounding the LLM in a structured scene graph and restricting its action space to high‑level primitives, thereby reducing hallucinations.

Semantic scene graphs have emerged as powerful data structures for representing 3D environments.  Hydra \cite{Hydra2021} constructs a multi‑layered dynamic scene graph that fuses odometry, mapping and semantic segmentation to enable long‑term spatial reasoning.  Similarly, semantic mapping systems such as Open Vocabulary Mapping (OVM) and Semantic Graph Memory (SGM) maintain object‑level information to support downstream tasks.  MoMa‑LLM extends these ideas by incorporating a probabilistic door model and an object‑to‑room assignment that accounts for 3D distances, resulting in a two‑level graph tailored to mobile manipulation.

Another relevant line of work focuses on open‑vocabulary perception using models such as CLIP, SAM and open‑vocabulary object detectors.  These models recognise arbitrary classes by leveraging language embeddings and enable the detection of previously unseen objects.  MoMa‑LLM leverages such detectors to populate its scene graph with object categories and to generate language prompts for the LLM.  Finally, prior approaches to language‑guided mobile manipulation (e.g., Code as Policies and VAT‑LM) have demonstrated the ability of LLMs to generate robot code or symbolic plans.  MoMa‑LLM differs by combining a learned high‑level planner with a rule‑based scene representation and explicit action primitives, facilitating interactive search in unknown environments.
